思路是使用**爬虫**，爬取学校官网源码上的所有网址，使用*正则表达式*从中提取出学校官网内的子域名。然后爬取每个子域名内所有网址，从中提取出学校官网子域名......依此递归。

先写爬取一个网址源码上所有网址，并提取子域名的函数：

```python
all_url = []

def find_Subdomain(url):
	r = requests.get(url).content.decode()
	soup = BeautifulSoup(r, 'html.parser')
	links = soup.find_all('a')
	links = list(set(links))#去除重复的标签
	for item in links:
		temp_url = item.get('href')
        '''如果网址包含“http://(.*)cqupt.edu.cn”，则保留'''
		if(re.match( r'http://(.*)cqupt.edu.cn',str(temp_url)) != None):
            '''去除“http://(.*)cqupt.edu.cn”后面的路径名'''
			temp_url = re.sub(r'cqupt.edu.cn(.*$)','cqupt.edu.cn',temp_url)
            '''如果temp_url已经存在于all_url列表中，则不保存'''
 			if temp_url in all_url:
				continue
			else:
				all_url.append(temp_url)
```

然后加入递归，得到所有子域名。总代码：

```python
from bs4 import BeautifulSoup
import requests
import re

all_url = []

def find_Subdomain(url):
	r = requests.get(url).content.decode()
	soup = BeautifulSoup(r, 'html.parser')
    
	links = soup.find_all('a')
	links = list(set(links))#去除重复的标签
    
	for item in links:
		temp_url = item.get('href')
        '''如果网址包含“http://(.*)cqupt.edu.cn”，则保留'''
		if(re.match( r'http://(.*)cqupt.edu.cn',str(temp_url)) != None):
            '''temp_url为除去“http://(.*)cqupt.edu.cn”后面的路径名的url'''
			temp_url = re.sub(r'cqupt.edu.cn(.*$)','cqupt.edu.cn',temp_url)
             '''如果temp_url已经存在于all_url列表中，则不执行保存，递归操作'''
			if temp_url in all_url:
				continue
			else:
				all_url.append(temp_url)
				print(temp_url)
                '''使用temp_url再次调用find_Subdomain()函数，实现递归'''
				try:
					find_Subdomain(temp_url)
				except:
					print("can't open " + temp_url)

url  = "http://www.cqupt.edu.cn"
find_Subdomain(url)
print(all_url)
```

